{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling - Job Domains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19001, 24)\n",
      "(18892, 24)\n",
      "Removed 109 duplicates (based on jobpost + Title)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import logging\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "def preprocess(tokens):\n",
    "    tokens_nop = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens_nop = [t.lower() for t in tokens_nop]\n",
    "    # wnl = nltk.WordNetLemmatizer()\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(\n",
    "        ['armenian', 'armenia', 'job', 'title', 'position', 'location', 'responsibilities', 'application', 'procedures',\n",
    "         'deadline', 'required', 'qualifications', 'renumeration', 'salary', 'date', 'company', 'yerevan',\n",
    "         'eligibility', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september',\n",
    "         'october', 'november', 'december'])\n",
    "    tokens_nostop = [t for t in tokens_nop if t not in stop]\n",
    "    # tokens_lem = [wnl.lemmatize(t) for t in tokens_nostop]\n",
    "    tokens_clean = [t for t in tokens_nostop if len(t) >= 3]  # simple way to remove the offending \" punctuations\n",
    "    return tokens_clean\n",
    "\n",
    "\n",
    "def plotWC(tokens):\n",
    "    text_clean = \" \".join(tokens)\n",
    "    print(text_clean)\n",
    "    wc = WordCloud(background_color=\"white\").generate(text_clean)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    fd = nltk.FreqDist(tokens)  # case sensitive!\n",
    "    fd.plot(50)\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('data job posts.csv')\n",
    "print(df1.shape)\n",
    "df = df1.drop_duplicates(['jobpost', 'Title'])\n",
    "print(\"Removed {0} duplicates (based on jobpost + Title)\".format(df1.shape[0] - df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub('\\S*@\\S*\\s?', '', x))  # remove emails\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub('\\s+', ' ', x))  # remove newlines\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub(\"\\'\", \"\", x))  # remove single quotes\n",
    "re1 = '(www)'  # Word 1\n",
    "re2 = '(\\\\.)'  # Any Single Character 1\n",
    "re3 = '((?:[a-z][a-z0-9_]*))'  # Variable Name 1\n",
    "re4 = '(\\\\.)'  # Any Single Character 2\n",
    "re5 = '((?:[a-z][a-z0-9_]*))'  # Variable Name 2\n",
    "rg = re.compile(re1 + re2 + re3 + re4 + re5, re.IGNORECASE | re.DOTALL)\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub(rg, \"\", x))\n",
    "re1 = '((?:[a-z][a-z0-9_]*))'  # Variable Name 1\n",
    "re2 = '(\\\\.)'  # Any Single Character 1\n",
    "re3 = '((?:[a-z][a-z0-9_]*))'  # Word 1\n",
    "rg = re.compile(re1 + re2 + re3, re.IGNORECASE | re.DOTALL)\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub(rg, \"\", x))\n",
    "df.jobpost = df.jobpost.apply(lambda x: re.sub('(\\\\d+)', \"\", x))  # remove numbers\n",
    "\n",
    "df['jobpost_token'] = df.jobpost.map(word_tokenize)\n",
    "df['jobpost_len'] = df.jobpost_token.apply(len)\n",
    "df['jobpost_token_uniq'] = df.jobpost_token.apply(set)\n",
    "df['jobpost_processed'] = df.jobpost_token.apply(preprocess)\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df['jobpost_processed'], min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[df['jobpost_processed']], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "df.jobpost_processed = make_bigrams(df.jobpost_processed)\n",
    "df.jobpost_processed = lemmatization(df.jobpost_processed, allowed_postags=['NOUN', 'VERB'])  # 'ADJ',, 'ADV'])\n",
    "\n",
    "dictionary = corpora.Dictionary(df['jobpost_processed'])\n",
    "#print(dictionary)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.119975545), (1, 0.10838561), (2, 0.42120668), (3, 0.020670587), (5, 0.018573191), (6, 0.30967)]\n",
      "[(0, 0.5529387), (1, 0.19301182), (2, 0.036064904), (5, 0.11257046), (6, 0.09716331)]\n",
      "[(0, 0.21326159), (1, 0.3510972), (2, 0.050571993), (5, 0.2761624), (6, 0.0974382)]\n",
      "[(0, 0.052259926), (1, 0.49375182), (2, 0.0112226475), (3, 0.047561854), (4, 0.02636911), (5, 0.023485646), (6, 0.34534898)]\n",
      "[(0, 0.29344884), (1, 0.032398142), (2, 0.0798206), (3, 0.36920053), (4, 0.06531269), (5, 0.07589083), (6, 0.08392841)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_num = 7\n",
    "\n",
    "# Use the dictionary to prepare a DTM (using TF)\n",
    "dtm_train = [dictionary.doc2bow(d) for d in df['jobpost_processed']]\n",
    "lda = gensim.models.ldamodel.LdaModel(dtm_train, num_topics=topic_num, alpha='auto', chunksize=30, id2word=dictionary,\n",
    "                                      passes=20, random_state=432)\n",
    "lda.show_topics()\n",
    "lda.show_topics(num_words=20)\n",
    "\n",
    "dtopics_train = lda.get_document_topics(dtm_train)\n",
    "# print topic distribution for 1st 5 rows\n",
    "for i in range(0, 5):\n",
    "    print(dtopics_train[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"ability\" + 0.021*\"communication\" + 0.019*\"customer\" + 0.018*\"term\" + 0.015*\"sale\" + 0.015*\"line\" + 0.015*\"llc\" + 0.014*\"marketing\" + 0.013*\"service\" + 0.012*\"send\"'),\n",
       " (1,\n",
       "  '0.066*\"project\" + 0.036*\"development\" + 0.022*\"program\" + 0.019*\"activity\" + 0.018*\"support\" + 0.018*\"implementation\" + 0.013*\"sector\" + 0.013*\"include\" + 0.012*\"community\" + 0.011*\"ensure\"'),\n",
       " (2,\n",
       "  '0.024*\"report\" + 0.016*\"bank\" + 0.015*\"datum\" + 0.014*\"prepare\" + 0.012*\"indicate\" + 0.011*\"year\" + 0.011*\"accounting\" + 0.010*\"finance\" + 0.010*\"branch\" + 0.010*\"account\"'),\n",
       " (3,\n",
       "  '0.039*\"development\" + 0.037*\"software\" + 0.033*\"design\" + 0.025*\"team\" + 0.022*\"developer\" + 0.018*\"web\" + 0.017*\"develop\" + 0.016*\"test\" + 0.015*\"system\" + 0.015*\"technology\"'),\n",
       " (4,\n",
       "  '0.034*\"engineering\" + 0.022*\"construction\" + 0.018*\"safety\" + 0.017*\"test\" + 0.014*\"engineer\" + 0.014*\"store\" + 0.013*\"site\" + 0.012*\"register\" + 0.011*\"amd\" + 0.011*\"answer\"'),\n",
       " (5,\n",
       "  '0.018*\"office\" + 0.017*\"course\" + 0.015*\"training\" + 0.012*\"expert\" + 0.011*\"study\" + 0.010*\"student\" + 0.010*\"material\" + 0.010*\"education\" + 0.009*\"medium\" + 0.009*\"month\"'),\n",
       " (6,\n",
       "  '0.035*\"management\" + 0.025*\"ensure\" + 0.023*\"business\" + 0.020*\"manage\" + 0.019*\"plan\" + 0.017*\"develop\" + 0.016*\"manager\" + 0.013*\"system\" + 0.013*\"service\" + 0.012*\"process\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 7 topics\n",
    "\n",
    "## 0- Sales and Marketing \n",
    "\n",
    "## 1- Project Management and Development \n",
    "\n",
    "## 2- Banking and Finance\n",
    "\n",
    "## 3- Software Development \n",
    "\n",
    "## 4- Construction and Safety Engineering \n",
    "\n",
    "## 5- Education and Training \n",
    "\n",
    "## 6- Business Development and Management \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
